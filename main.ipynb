{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MACHINE LEARING PROJECT**\n",
    "*by Brusati Lorenzo, nÂ°535355*\n",
    "\n",
    "This project aims to develop a comprehensive machine learning pipeline to predict smoking habits using various features. In the following sections, we outline the process from dataset acquisition to data preprocessing, model selection, and the final refinement of our best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "879938d90ee147fb8acd97e8c37c440a",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988329110,
    "source_hash": "7f5397b1"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures, FunctionTransformer\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    Perceptron, LogisticRegression, LinearRegression, SGDClassifier, \n",
    "    PassiveAggressiveClassifier, RidgeClassifier, RidgeClassifierCV\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlxtend.classifier import (\n",
    "    Adaline, SoftmaxRegression, MultiLayerPerceptron, StackingClassifier, \n",
    "    StackingCVClassifier, EnsembleVoteClassifier\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, matthews_corrcoef, f1_score, classification_report, confusion_matrix, roc_auc_score,\n",
    "    accuracy_score, roc_curve, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay, precision_recall_curve\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, RepeatedStratifiedKFold, \n",
    "    cross_val_score, cross_validate, learning_curve, validation_curve, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "\n",
    "from scipy.stats import loguniform, beta, uniform\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading and Analyzing the Dataset**\n",
    "\n",
    "In this section, we begin by importing the essential libraries for data handling, visualization, and machine learning. The dataset is retrieved from my personal GitHub repository using the `requests` module, which leverages a GitHub token for authentication.\n",
    "\n",
    "Key steps include:\n",
    "- **Loading Data into a DataFrame:** The CSV content is read using `pd.read_csv` after wrapping the text in a `StringIO` object, this ensures the dataset is not effectively downloaded locally but will be stored in the variable `dataset` for the whole project study.\n",
    "- **Dataset Overview:** The `dataset` call provides a visualization of the DataFrame, with the number of rows, columns, and data types, ensuring that we understand the dataset's structure before moving on to further analysis. In fact, the dataset provide column with inforations about people body features, blood testsand and a final target column that let us know if the person has smoking habits or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e981bb8fb93944619fdc3a3cd14b47b0",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 402,
    "execution_start": 1735988205697,
    "source_hash": "1f580a81"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "file_name = \"dataset_10000.csv\"\n",
    "\n",
    "file_url = f\"https://raw.githubusercontent.com/brusati04/smoking_ml_project/main/{file_name}\"\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "response = requests.get(file_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    from io import StringIO\n",
    "    csv_dataset = StringIO(response.text)\n",
    "    dataset = pd.read_csv(csv_dataset)\n",
    "else:\n",
    "    print(\"Download failed\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "\n",
    "Before splitting the data into training and testing sets, we perform several preprocessing steps to ensure that the dataset is clean and well-prepared for modeling. This section addresses missing values, potential imbalances, unnecessary information and data transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Target Distribution Analysis:**  \n",
    "  We compute the normalized counts of the target variable (`smoking`) using `value_counts(normalize=True)`. This analysis helps identify class imbalances which could affect model performance, fortunately the model has a perfect balance between smokers and non smokers people. This ensure the model will have enought datas to learn from, in order to correctly distinguish the binary target lablel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "55b00b90d69742ea9d744092ee323d17",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988206326,
    "source_hash": "bd23577c"
   },
   "outputs": [],
   "source": [
    "# Analyze the balance of our target label\n",
    "distr = dataset[\"smoking\"].value_counts(normalize=True)\n",
    "print(f\"Distribuzione della variabile target: {distr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Inspection and Handling of Missing Values:**\n",
    "  Each column is examined for missing data by iterating through the columns and counting the number of `NaN` entries. Rows with more than a specified threshold of missing values (e.g., more than 2 missing values) are dropped using `dropna`. This ensures that the remaining data is robust enough for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analize the distribution of Nan values:\n",
    "print(\"Missing values in the dataset:\")\n",
    "for col in dataset.columns:\n",
    "    nan_count = dataset[col].isnull().sum()\n",
    "    print(f\"{col}: {nan_count}\")\n",
    "\n",
    "# Set threshold\n",
    "n = 2\n",
    "num_rows = (dataset.isna().sum(axis=1) > n).sum()\n",
    "\n",
    "dataset = dataset.dropna(axis=0, thresh=len(dataset.columns)-2)\n",
    "\n",
    "print(f\"Number of rows with more than {n} NaN values: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Dominant Category Check:** \n",
    "  For each column, the ratio of the most frequent category is computed. Columns where a single category dominates (ratio greater than 0.9) are dropped, reducing redundancy and dimensionality in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of the most common category to all columns\n",
    "dominant_ratios = dataset.apply(lambda series: series.value_counts(normalize=True).iloc[0])\n",
    "print(dominant_ratios)\n",
    "\n",
    "# Let's drop the columns with a dominant category ratio greater than 0.9:\n",
    "dataset = dataset.drop(columns=dominant_ratios[dominant_ratios > 0.9].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Visualization of Missing Data:**  \n",
    "  The `missingno` library is used to generate a matrix plot that visually represents the presence of missing values. This visualization assists us in understanding the overall data quality that we reached until now and pointing areas that require further attention.\n",
    "  we can notice there are overall Nan values are uniformly distributed, so there aren't particular colums to be dropped. we have to manage those Nan values implementing a imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Data Transformation Pipeline Setup:**  \n",
    "  A series of transformations is prepared using scikit-learn `Pipeline`:\n",
    "\n",
    "  - **Scaling Numerical Features:** The `age` column is scaled using `MinMaxScaler` to normalize its range.\n",
    "  \n",
    "  - **Encoding Categorical Variables:**  For categorical features such as `gender` and `tartar`, a two-step process is implemented:\n",
    "    1. **Imputation:** Missing values are first handled using `SimpleImputer` with the strategy set to \"most_frequent\".\n",
    "    \n",
    "    2. **Ordinal Encoding:** The `OrdinalEncoder` is then applied, converting the categorical values into numerical form. The encoding is explicitly defined (e.g., `[\"F\", \"M\"]` for gender and `[\"N\", \"Y\"]` for tartar) to maintain consistency.\n",
    "  \n",
    "  - **Standardizing Body Signals:** A similar pipeline is constructed for all the blood tests columns features, ensuring they are standardized for better model performance with `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8b656d0c6e9d4801b9736dd28ceee4b1",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 875,
    "execution_start": 1735988206406,
    "source_hash": "bf555741"
   },
   "outputs": [],
   "source": [
    "minmax_age = MinMaxScaler()\n",
    "\n",
    "oe_tartar = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\",  OrdinalEncoder(categories=[[\"N\",\"Y\"]]))\n",
    "        ])\n",
    "\n",
    "oe_gender = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\", OrdinalEncoder(categories=[[\"F\",\"M\"]]))\n",
    "        ])\n",
    "\n",
    "body_signals = dataset.columns.drop([\"ID\", \"gender\", \"age\" ,\"dental caries\", \"tartar\", \"smoking\"]).tolist()\n",
    "std_body_signals = Pipeline([\n",
    "        (\"pipe_sim\", KNNImputer(n_neighbors=4)),\n",
    "        (\"pipe_std\", StandardScaler())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Combining Transformations:**  \n",
    "  All individual pipelines are integrated into a single `ColumnTransformer` that applies the appropriate transformation to each set of columns. This unified approach ensures consistency and simplifies the application of preprocessing steps to both the training and testing datasets.\n",
    "  remaining features will be passed thanks to `remainder=\"passthrough\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN TRASFORMATION\n",
    "\n",
    "smoking_tr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"id\", \"drop\", [\"ID\"]),\n",
    "        (\"gender\", oe_gender, [\"gender\"]),\n",
    "        (\"age\", minmax_age, [\"age\"]),\n",
    "        (\"body_signals\", std_body_signals, body_signals),\n",
    "        (\"tartar\", oe_tartar, [\"tartar\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\", # not applying anythig to remaining features (i.e. dental caries) \n",
    "    sparse_threshold=1,\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Data Transformation Pipeline Setup:**  \n",
    "Here we provide a final visualization of the dataset, just to check that all the transformation worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datatset = pd.DataFrame(smoking_tr.fit_transform(dataset), columns=smoking_tr.get_feature_names_out())\n",
    "final_datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "msno.matrix(final_datatset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train and Test Split**\n",
    "\n",
    "Now let's finally jump right into where the real work begins. After preprocessing, the next critical step is to split the dataset into training and testing sets. This section outlines how we separate the data and prepare it for model training and evaluation:\n",
    "\n",
    "- **Feature and Target column Separation:**  \n",
    "  The dataset is divided into features (`X`) and the target variable (`y`). The target is the `smoking` column, that is a binary 0-1 column. so we are dealing with \n",
    "\n",
    "- **Splitting the Data:**  \n",
    "  The `train_test_split` function from scikit-learn is used to divide the data. The key parameters include:\n",
    "  - `test_size`: percentage of the data reserved for testing.\n",
    "  - `stratify=y`: Ensures that the target variableâs distribution is maintained in both the training and testing sets, which is especially important when dealing with imbalanced classes.\n",
    "  - `random_state=42` and `shuffle=True`: These parameters guarantee reproducibility and ensure iid: indipendent and identically distributed (shuffling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature e target columns\n",
    "X = dataset.drop(columns=[\"smoking\"])\n",
    "y = dataset[\"smoking\"]\n",
    "\n",
    "# train & test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train==1)/len(y_train) , sum(y_test==1)/len(y_test) # sanity check of stratified y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Applying the Transformation Pipeline:**  \n",
    "  After splitting, the transformation pipeline `smoking_tr` is applied to both the training and test sets. From now on, we assume that our data have been correctly preprocessed and transformed.\n",
    "\n",
    "- **Initial Pipeline Setup:**  \n",
    "  We have to define the candidate models and select the best candidates through the nested-cross validation process which combine hyperparameter optimization and model selection into a single block of code. In order to do that, we configured a model pipeline using an imbalanced data handling pipeline (`IMBPipeline`), which incorporates:\n",
    "  - **Sampler:** for handling class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "  - **Dimensionality Reduction:** for reducing feature dimensionality and improving computational efficiency.\n",
    "  - **Classifier:** for making predictions based on the transformed dataset.\n",
    "\n",
    "Let's proceed by defining a starting model or pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smoking_tr.fit_transform(X_train)\n",
    "X_test = smoking_tr.transform(X_test)\n",
    "\n",
    "model_pipeline = IMBPipeline([\n",
    "    ('sampler', None),\n",
    "    ('dim_reduction', None),\n",
    "    ('classifier', None)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**\n",
    "\n",
    "This section is dedicated to exploring, between various machine learning models and their hyperparameters configurations, which performs better for our dataset. Given the complexity of the task, multiple aspects of the model pipeline are tuned:\n",
    "- **Sampler Configurations:**  \n",
    "  - No sampling.\n",
    "  - SMOTE - Synthetic Minority Over-sampling Technique - with various sampling strategies (i.e., focusing on the minority class or specific ratios).\n",
    "  - RandomOverSampler with similar strategies.\n",
    "\n",
    "- **Dimensionality Reduction Techniques:**  \n",
    "  - No dimensionality reduction.\n",
    "  - PCA with different levels of variance retention.\n",
    "  - Linear Discriminant Analysis (LDA).\n",
    "\n",
    "- **Classifier Options and Their Hyperparameters:**  \n",
    "  A wide range of classifiers, studied in class, is considered:\n",
    "  - **Perceptron** classifier is included with varying numbers of iterations, regularization penalties (`l1` and `l2`), and learning rates to adjust the step size during training. Early stopping is enabled to prevent unnecessary computations when convergence is reached.\n",
    "  - **K-Nearest Neighbors (KNN)** classifier is configured with different values for the number of neighbors, weight functions (`uniform` and `distance`), and algorithm choices (`auto`, `ball_tree`, `kd_tree`, `brute`) to optimize performance based on the dataset structure.\n",
    "  - **Random Forest** classifier is tested with different numbers of estimators (trees) and varying maximum depths to balance complexity and overfitting.\n",
    "  - **AdaBoost** classifier is implemented with a base decision tree (using entropy criterion and depth of 1) and explores different numbers of estimators along with varied learning rates.\n",
    "  - **Logistic Regression** model includes a wide range of regularization strengths (log-spaced), multiple penalty options (`l1`, `l2`, `elasticnet`, `None`), and different solvers (`liblinear`, `saga`) to handle optimization efficiently.\n",
    "  - **Support Vector Classifier (SVC)** is tested with a linear kernel, multiple values for the regularization parameter `C`, and different gamma values to control the influence of training points. The tolerance and maximum iterations are also varied to fine-tune convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_configs = [\n",
    "    {'sampler': [None]},\n",
    "    {'sampler': [SMOTE()], 'sampler__sampling_strategy': ['minority', 1.2, 0.9, 0.7]},\n",
    "    {'sampler': [RandomOverSampler()], 'sampler__sampling_strategy': ['minority', 1.2, 0.9, 0.7]},\n",
    "]\n",
    "\n",
    "dim_reduction_configs = [\n",
    "    {'dim_reduction': [None]},\n",
    "    {'dim_reduction': [LDA()]},\n",
    "    {'dim_reduction': [PCA()], 'dim_reduction__n_components': [None, 0.95, 0.99]}\n",
    "]\n",
    "\n",
    "classifier_configs = [\n",
    "    {\n",
    "        'classifier': [Perceptron()],\n",
    "        'classifier__max_iter': [100, 500, 1000],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__eta0': [1, 0.1, 0.01, 0.001],\n",
    "        'classifier__early_stopping': [True],\n",
    "    },\n",
    "    {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__n_estimators': [100, 200, 500, 1000, None],\n",
    "        'classifier__max_depth': [10, 20, 30, None]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [LogisticRegression()],\n",
    "        \"classifier__C\": np.logspace(-4, 4, 20),\n",
    "        \"classifier__penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"classifier__solver\": [\"liblinear\", \"saga\"],\n",
    "        \"classifier__max_iter\": [100, 500, 1000, 5000],\n",
    "        \"classifier__l1_ratio\": np.linspace(0, 1, 10)\n",
    "    },\n",
    "    {\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear'],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'classifier__tol': [1e-4, 1e-3, 1e-2],\n",
    "        'classifier__max_iter': [100, 1000, 10000, 10000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Evaluation on all possible configurations:**  \n",
    "  The different configurations are combined using `itertools.product` to generate a complete grid of possible settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs = [dict(itertools.chain(*(e.items() \n",
    "for e in configuration)))\n",
    "for configuration in itertools.product(sampler_configs,dim_reduction_configs,classifier_configs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Number of all possible configurations: {len(all_configs)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **RandomizedSearchCV:**  \n",
    "    This search strategy is employed to efficiently explore the hyperparameter space by sampling a fixed number of candidates from the entire grid. The search is optimized using cross-validation (with the F1 score as the performance metric) to ensure that the model generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(model_pipeline,\n",
    "    param_distributions=all_configs,\n",
    "    n_iter=len(all_configs) * 10,\n",
    "    n_jobs=-1,\n",
    "    cv = 2,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Cross-Validation:**\n",
    "    The output of cross_validate is a dictionary providing all the information for each cross-validation iteration:\n",
    "    The cross-validation results, including the best configurations for each component (sampler, dimensionality reduction, and classifier), are analyzed in detail. The F1 scores for both training and test sets are computed to assess model performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=rs, X=X_train, y=y_train, scoring='f1', cv = 5, return_estimator=True, verbose=3) # 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    print(estimator.best_estimator_.get_params()['sampler'])\n",
    "    print(estimator.best_estimator_.get_params()['dim_reduction'])\n",
    "    print(estimator.best_estimator_.get_params()['classifier'])\n",
    "    print(scores['test_score'][index])\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in scores['estimator']:\n",
    "    model_name = estimator.best_estimator_.get_params()['classifier'].__class__.__name__\n",
    "     \n",
    "    # Train the model and predict on training and test sets\n",
    "    pred_train = estimator.best_estimator_.fit(X_train, y_train).predict(X_train)\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Calculate F1-scores\n",
    "    f1_train = f1_score(y_train, pred_train)\n",
    "    f1_test = f1_score(y_test, pred_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'F1 on Training Set: {f1_train:.4f}, F1 on Test Set: {f1_test:.4f}')\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **Comment:**\n",
    "    The output of cross_validate give us also other models (i.e. SVC). In my current study i decided to choose Logistic Regression because was the one with the best confusion matrix, and also because has least overfitting with respect to SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Refinement of the Selected Model**\n",
    "From the cross validation we obtained that the most effective model is Logistic Regression, here we have a further analisys of the f1-score for the train and test evaluation.\n",
    "But now let's refine further the hyperparameter tuting of our model in order to gain deeper insights into its behavior and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pipeline = IMBPipeline([\n",
    "    ('sampler', None),\n",
    "    ('dim_reduction', None),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\", None],\n",
    "    \"classifier__C\": [0.0001, 0.001, 0.01],\n",
    "    \"classifier__solver\": [\"liblinear\"],\n",
    "    \"classifier__l1_ratio\": np.linspace(0, 0.5, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we proceede as before, but instead of using a cross validation, we re-run the hyperparameter optmization fixing the modelling apporach and varying the hyperparameters in a range close to selected model's one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = RandomizedSearchCV(\n",
    "    estimator= best_model_pipeline,\n",
    "    param_distributions = params,\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2),\n",
    "    n_iter=20,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_best.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, rs_best.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = rs_best.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, rs_best.predict(X_test), cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the confusion matrix is good also because in our medical diagnosis, it's better to have a false positive than a false negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Analysis of the Graphs**\n",
    "\n",
    "In this section, we take a closer look at the visualizations generated during model refinement and performance evaluation: Overall, the visual analysis confirms that while the model performs robustly, fine-tuning and further data augmentation could enhance its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification report for train and test sets\n",
    "report_train = classification_report(y_train, cls.predict(X_train), output_dict=True)\n",
    "report_test = classification_report(y_test, cls.predict(X_test), output_dict=True)\n",
    "\n",
    "# Extract F1-score (assuming binary classification and '1' is the positive class)\n",
    "f1_train = report_train[\"1\"][\"f1-score\"]\n",
    "f1_test = report_test[\"1\"][\"f1-score\"]\n",
    "\n",
    "# Compute Generalization Gap\n",
    "generalization_gap = f1_train - f1_test\n",
    "overfitting_ratio = f1_train / f1_test if f1_test > 0 else np.inf\n",
    "\n",
    "# Print results\n",
    "print(\"Classification Report (Train):\\n\", classification_report(y_train, cls.predict(X_train)))\n",
    "print('')\n",
    "print(\"Classification Report (Test):\\n\", classification_report(y_test, cls.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1-score (Train): {f1_train:.4f}\")\n",
    "print(f\"F1-score (Test): {f1_test:.4f}\")\n",
    "print(f\"Generalization Gap: {generalization_gap:.4f}\")\n",
    "print(f\"Overfitting Ratio: {overfitting_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Learning Curve: \n",
    "  The learning curves illustrate how the F1-score evolves as the training set size increases. The training curve's steady convergence indicates that the model is capturing the underlying patterns well. However, the gap between the training and validation curves suggests a minor degree of overfitting. This gap could potentially be reduced by incorporating additional data or applying stronger regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(cls,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                                       cv = 5,\n",
    "                                                       n_jobs = -1,\n",
    "                                                       scoring = 'f1',\n",
    "                                                       shuffle = False)\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='+',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='d', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.7, 0.8])\n",
    "ax.set_xlim([train_sizes[0], train_sizes[-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Validation Curve:  \n",
    "  By plotting the validation curve for the hyperparameter `C`, we observe the model's sensitivity to changes in regularization strength. The curve highlights an optimal range where the performance peaks, providing clear guidance on tuning this parameter. The confidence intervals around these curves also reflect the model's stability across different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_scores, test_scores = validation_curve(cls,\n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        param_range= range_C, \n",
    "        param_name='classifier__C',\n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        scoring='f1'\n",
    ")\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_C, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_C, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Parameter C')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim([0.7, 0.8])\n",
    "ax.set_xlim([range_C[0], range_C[-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Precision-Recall Curve (Threshold Analysis):\n",
    "\n",
    "This code block calculates precision and recall scores for various thresholds based on model scores.\n",
    "It then plots both precision and recall as functions of the decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_cls = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_cls.decision_function([X_train[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = lg_cls.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[:10] # A sample of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_train, scores)\n",
    "\n",
    "threshold = 0.9639675188737687\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", lw=2)\n",
    "ax.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", lw=2)\n",
    "ax.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()  # first index â¥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.grid()\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(recalls, precisions, lw=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "ax.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "ax.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "ax.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=f\"Point at threshold {threshold}\")\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.axis([0, 1, 0, 1])\n",
    "ax.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_for_80_precision = thresholds[(precisions >= 0.80).argmax()]\n",
    "threshold_for_80_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Other Visual Metrics:**  \n",
    "  Additional plots, such as confusion matrices and ROC curves, offer insights into the modelâs classification performance. The confusion matrix confirms that the model is accurately distinguishing between the classes, while the ROC curve and precision-recall plots demonstrate strong performance even in the presence of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - ROC Curve:\n",
    "This code block computes and plots the Receiver Operating Characteristic (ROC) curve.\n",
    "The false positive rates (FPR) and recalls (used here as a proxy for the true positive rate) are calculated.\n",
    "The ROC curve of a random classifier (diagonal line) is also plotted for reference,\n",
    "helping to assess the classifierâs ability to distinguish between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs, recalls, thresholds = roc_curve(y_train, scores)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(fprs, recalls, linewidth=2, label=\"ROC curve\")\n",
    "ax.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "ax.set_xlabel('False Positive Rate - FPR')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.axis([0, 1, 0, 1])\n",
    "ax.legend(loc=\"lower right\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When i want to get ad overall evaluation of my classifier model i used, i have to look at:\n",
    "ROC curve, Area under the curve and f1-score.\n",
    "\n",
    "Area under the curve: the closer it looks to a square, the better is.\n",
    "\n",
    "Attention: for ADALINE and Perceptron, we cannot evaluate ROC curve on Loss function, we can just work on f1-score (the precision-recall relation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusions**\n",
    "\n",
    "This project has successfully built a comprehensive machine learning pipeline to predict smoking habits using a medium-size dataset. The end-to-end workflowâfrom data loading and preprocessing to model selection, tuning, and final evaluationâhas demonstrated several key insights:\n",
    "\n",
    "- **Robust Data Handling:**  \n",
    "  The initial exploratory analysis and rigorous preprocessing steps, including missing value imputation and the thoughtful design of a data transformation pipeline, ensured that the input data was of high quality. This foundational work is crucial for the success of any predictive model.\n",
    "\n",
    "- **Diverse Model Exploration:**  \n",
    "  By testing multiple classifiers and hyperparameter configurations (from SGD and Logistic Regression to ensemble methods like Random Forest and advanced boosting algorithms), the project provided a rich comparative analysis. This not only highlighted the strengths and weaknesses of different approaches but also helped identify the optimal model configuration.\n",
    "\n",
    "- **Visual Insights for Model Refinement:**  \n",
    "  The learning and validation curves, along with other graphical analyses, provided actionable insights into the model's performance. For instance, the slight divergence between training and validation performance pointed to potential areas for improvement, such as reducing overfitting through enhanced regularization or expanding the dataset.\n",
    "\n",
    "- **Practical Applications and Future Enhancements:**  \n",
    "  Beyond the technical achievements, this project has significant real-world implications. The predictive model could be used in healthcare settings for early identification of individuals at risk, enabling targeted interventions and personalized health recommendations. Additionally, the methodology could be adapted for related fields such as lifestyle analytics, public health monitoring, and even insurance risk assessment.\n",
    "\n",
    "Looking ahead, future work could focus on:\n",
    "- **Enhanced Feature Engineering:** Incorporating additional data sources or domain-specific features to further improve model accuracy.\n",
    "- **Advanced Modeling Techniques:** Exploring deep learning architectures or more sophisticated ensemble methods to capture non-linear relationships.\n",
    "\n",
    "In summary, while the current results are promising, there remains ample opportunity for refinement and extension, ensuring that the model not only performs well in a controlled setting but also delivers practical value in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEGLIO UN FALSO POSITIVO CHE UN FALSO NEGATIVO"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "e86ca8c4e97c48a29ff14cb04bf74249",
  "deepnote_persisted_session": {
   "createdAt": "2025-01-04T10:59:37.017Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
