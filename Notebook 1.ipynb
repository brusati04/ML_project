{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "cell_id": "879938d90ee147fb8acd97e8c37c440a",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988329110,
    "source_hash": "7f5397b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (precision_score, recall_score, matthews_corrcoef , f1_score, classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import (learning_curve, validation_curve, train_test_split, KFold, StratifiedKFold, \n",
    "                                    cross_val_score, GridSearchCV, RandomizedSearchCV, cross_validate, RepeatedStratifiedKFold)\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.stats import loguniform, beta, uniform\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE; is the union of several transformers or imputer - for nan values\n",
    "SIMPLE IMPUTER: IF HAVE MISSING DATATS: FILL WITH MOST FREQUENT\n",
    "\n",
    "ORDINAL ENCODER: AS ONEHOT BUT WITH ORDER - COL: GENDER(MALE - FEMALE) , TARTAR(Y - N) , ORAL (=> CAN BE DROPPED)\n",
    "\n",
    "MINMAX SCALER OR STANDARD SCALER: FOR NUMERICAL DATAS: ALL COLUMNS with standard scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e981bb8fb93944619fdc3a3cd14b47b0",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 402,
    "execution_start": 1735988205697,
    "source_hash": "1f580a81"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "file_name = \"dataset_1.csv\"\n",
    "file_url = f\"https://raw.githubusercontent.com/brusati04/smoking_ml_project/main/{file_name}\"\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "response = requests.get(file_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    from io import StringIO\n",
    "    csv_dataset = StringIO(response.text)\n",
    "    dataset = pd.read_csv(csv_dataset)\n",
    "else:\n",
    "    print(\"Download failed\")\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "cell_id": "3a92f9b443cd4bb1aba9c650e17848af",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 110,
    "execution_start": 1735988206163,
    "source_hash": "b8fd64b1"
   },
   "outputs": [],
   "source": [
    "# if input(\"do you whant to finish quickly the project?(y/n):\") == \"y\":\n",
    "#     dataset.dropna(axis=1)\n",
    "#     dataset.info()\n",
    "#     print(\"well done, now 30L\")\n",
    "# else:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "55b00b90d69742ea9d744092ee323d17",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988206326,
    "source_hash": "bd23577c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analizziamo il bilanciamento della variabile target\n",
    "sm = dataset[\"smoking\"].value_counts(normalize=True)\n",
    "print(f\"Distribuzione della variabile target: {sm}\")\n",
    "\n",
    "# analizziamo la distribuzione dei Nan values:\n",
    "for col in dataset:\n",
    "    Nan=dataset[col].isnull().sum()\n",
    "    print(f\"missing values in {col}: {Nan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # Set your threshold\n",
    "num_rows = (dataset.isna().sum(axis=1) > n).sum()\n",
    "print(f\"Number of rows with more than {n} NaN values: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(axis=0, thresh=len(dataset.columns)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns not reported in the figure will be discarded.\n",
    "\n",
    "For features age and fare the pipeline is composed by two transformers:\n",
    "\n",
    "KNNImputer: both features contain missing values, so we have to apply an imputation strategy. In this case the strategy is based on the idea of \n",
    "k\n",
    "k-nearest neighbors.\n",
    "StandardScaler: both features are numerical\n",
    "For features pclass:\n",
    "\n",
    "An OrdinalEncoder transforms the strings '3','2' and '1' corresponding to the ticket classes into the numerical values 3,2 and 1.\n",
    "For features sex and embarked, we apply:\n",
    "\n",
    "SimpleImputer: feature embarked contains two missing values, while the column sex will be untouched. As a strategy we use 'most_frequent' since both features are categorical\n",
    "OneHotEncoder: features are categorical.\n",
    "For features sbsp and parch we define a customer transformer that builds a new feature is_alone indicating whether the passenger travelled alone or not. More details about how to code customer transformers in the following optional section.\n",
    "\n",
    "For feature name we define a further customer transformer to infer the title (Mr, Miss, Doc, Captain, etc..) from the fullname.\n",
    "\n",
    "ID: drop\n",
    "gender: ordinal, \n",
    "age: minmax,\n",
    "oral: ordinal\n",
    "dental caries: none\n",
    "tartar: ordinal\n",
    "O/W: standardization x 20, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "cell_id": "8b656d0c6e9d4801b9736dd28ceee4b1",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 875,
    "execution_start": 1735988206406,
    "source_hash": "bf555741"
   },
   "outputs": [],
   "source": [
    "minmax_age = MinMaxScaler()\n",
    "\n",
    "oe_oral = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\",  OrdinalEncoder(categories=[[\"N\",\"Y\"]]))\n",
    "        ])\n",
    "\n",
    "oe_tartar = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\",  OrdinalEncoder(categories=[[\"N\",\"Y\"]]))\n",
    "        ])\n",
    "\n",
    "oe_gender = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\", OrdinalEncoder(categories=[[\"F\",\"M\"]]))\n",
    "        ])\n",
    "\n",
    "std_body_signals = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_std\", StandardScaler())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN TRASFORMATION\n",
    "body_signals = [\n",
    "    \"height(cm)\",\"weight(kg)\",\"waist(cm)\",\"eyesight(left)\",\"eyesight(right)\",\n",
    "    \"hearing(left)\",\"hearing(right)\",\"systolic\",\"relaxation\",\"fasting blood sugar\",\n",
    "    \"Cholesterol\",\"triglyceride\",\"HDL\",\"LDL\",\"hemoglobin\",\"Urine protein\",\n",
    "    \"serum creatinine\",\"AST\",\"ALT\",\"Gtp\"\n",
    "]\n",
    "\n",
    "smoking_tr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"id\", \"drop\", [\"ID\"]),\n",
    "        (\"gender\", oe_gender, [\"gender\"]),\n",
    "        (\"age\", minmax_age, [\"age\"]),\n",
    "        (\"body_signals\", std_body_signals, body_signals),\n",
    "        (\"oral\", oe_oral, [\"oral\"]),\n",
    "        (\"tartar\", oe_tartar, [\"tartar\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\", \n",
    "    sparse_threshold=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separiamo feature e target columns:\n",
    "X = dataset.drop(columns=[\"smoking\"])\n",
    "y = dataset[\"smoking\"]\n",
    "\n",
    "# Divisione in train e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,  stratify = y, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train==1)/len(y_train) , sum(y_test==1)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = IMBPipeline([\n",
    "    ('trans', smoking_tr),\n",
    "    ('sampler', SMOTE()),\n",
    "    ('dim_reduction', PCA(n_components=0.8)),\n",
    "    ('classifier', Perceptron())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smoking_tr.fit_transform(X_train)\n",
    "X_test = smoking_tr.fit_transform(X_test)\n",
    "\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
    "model_perceptron = Perceptron(max_iter=1000, random_state=42, eta0=0.01)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "model_logistic = LogisticRegression(random_state=42, class_weight=\"balanced\")\n",
    "model_tree = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "model_lda = LDA()\n",
    "model_sgd = SGDClassifier(random_state=42, class_weight=\"balanced\")\n",
    "model_ridge = RidgeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "model_pa = PassiveAggressiveClassifier(random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "# creiamo una lista di tutti i modelli qui sopra:\n",
    "models = [model_rf, model_perceptron, model_knn, model_logistic, model_tree, model_lda, model_sgd, model_ridge, model_pa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_test).to_csv(\"X_test.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(f\"MODEL: {model}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_confusion_matrix = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred, cmap=plt.cm.Blues)\n",
    "    print(f\"accouracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "    print(f\"precision: {precision_score(y_test, y_test_pred)}\")\n",
    "    print(f\"recall: {recall_score(y_test, y_test_pred)}\")\n",
    "    print(f\"f1-score: {f1_score(y_test, y_test_pred)}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=77cb4bcc-2fc9-439b-9d91-de217bfdb267' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1).to_csv(\"test.csv\", index=False) # save test set"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "e86ca8c4e97c48a29ff14cb04bf74249",
  "deepnote_persisted_session": {
   "createdAt": "2025-01-04T10:59:37.017Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
