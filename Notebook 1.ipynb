{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_id": "879938d90ee147fb8acd97e8c37c440a",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988329110,
    "source_hash": "7f5397b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (precision_score, recall_score, matthews_corrcoef , f1_score, classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay)\n",
    "from sklearn.model_selection import (learning_curve, validation_curve, train_test_split, KFold, StratifiedKFold, \n",
    "                                    cross_val_score, cross_validate, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression, LinearRegression, SGDClassifier, PassiveAggressiveClassifier, RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.stats import loguniform, beta, uniform\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.classifier import Adaline, Perceptron, SoftmaxRegression, MultiLayerPerceptron, StackingClassifier, EnsembleVoteClassifier\n",
    "\n",
    "# iport Stocastic gradient decent from mlxtend_\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE; is the union of several transformers or imputer - for nan values\n",
    "SIMPLE IMPUTER: IF HAVE MISSING DATATS: FILL WITH MOST FREQUENT\n",
    "\n",
    "ORDINAL ENCODER: AS ONEHOT BUT WITH ORDER - COL: GENDER(MALE - FEMALE) , TARTAR(Y - N) , ORAL (=> CAN BE DROPPED)\n",
    "\n",
    "MINMAX SCALER OR STANDARD SCALER: FOR NUMERICAL DATAS: ALL COLUMNS with standard scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e981bb8fb93944619fdc3a3cd14b47b0",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 402,
    "execution_start": 1735988205697,
    "source_hash": "1f580a81"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "file_name = \"dataset_smoking.csv\"\n",
    "file_url = f\"https://raw.githubusercontent.com/brusati04/smoking_ml_project/main/{file_name}\"\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "response = requests.get(file_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    from io import StringIO\n",
    "    csv_dataset = StringIO(response.text)\n",
    "    dataset = pd.read_csv(csv_dataset)\n",
    "else:\n",
    "    print(\"Download failed\")\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_id": "3a92f9b443cd4bb1aba9c650e17848af",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 110,
    "execution_start": 1735988206163,
    "source_hash": "b8fd64b1"
   },
   "outputs": [],
   "source": [
    "# if input(\"do you whant to finish quickly the project?(y/n):\") == \"y\":\n",
    "#     dataset.dropna(axis=1)\n",
    "#     dataset.info()\n",
    "#     print(\"well done, now 30L\")\n",
    "# else:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "55b00b90d69742ea9d744092ee323d17",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988206326,
    "source_hash": "bd23577c"
   },
   "outputs": [],
   "source": [
    "# Analizziamo il bilanciamento della variabile target\n",
    "sm = dataset[\"smoking\"].value_counts(normalize=True)\n",
    "print(f\"Distribuzione della variabile target: {sm}\")\n",
    "\n",
    "\n",
    "# analizziamo la distribuzione dei Nan values:\n",
    "for col in dataset:\n",
    "    Nan=dataset[col].isnull().sum()\n",
    "    print(f\"missing values in {col}: {Nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # Set your threshold\n",
    "num_rows = (dataset.isna().sum(axis=1) > n).sum()\n",
    "print(f\"Number of rows with more than {n} NaN values: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(axis=0, thresh=len(dataset.columns)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns not reported in the figure will be discarded.\n",
    "\n",
    "For features age and fare the pipeline is composed by two transformers:\n",
    "\n",
    "KNNImputer: both features contain missing values, so we have to apply an imputation strategy. In this case the strategy is based on the idea of \n",
    "k\n",
    "k-nearest neighbors.\n",
    "StandardScaler: both features are numerical\n",
    "For features pclass:\n",
    "\n",
    "An OrdinalEncoder transforms the strings '3','2' and '1' corresponding to the ticket classes into the numerical values 3,2 and 1.\n",
    "For features sex and embarked, we apply:\n",
    "\n",
    "SimpleImputer: feature embarked contains two missing values, while the column sex will be untouched. As a strategy we use 'most_frequent' since both features are categorical\n",
    "OneHotEncoder: features are categorical.\n",
    "For features sbsp and parch we define a customer transformer that builds a new feature is_alone indicating whether the passenger travelled alone or not. More details about how to code customer transformers in the following optional section.\n",
    "\n",
    "For feature name we define a further customer transformer to infer the title (Mr, Miss, Doc, Captain, etc..) from the fullname.\n",
    "\n",
    "ID: drop\n",
    "gender: ordinal, \n",
    "age: minmax,\n",
    "oral: ordinal\n",
    "dental caries: none\n",
    "tartar: ordinal\n",
    "O/W: standardization x 20, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "8b656d0c6e9d4801b9736dd28ceee4b1",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 875,
    "execution_start": 1735988206406,
    "source_hash": "bf555741"
   },
   "outputs": [],
   "source": [
    "minmax_age = MinMaxScaler()\n",
    "\n",
    "oe_tartar = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\",  OrdinalEncoder(categories=[[\"N\",\"Y\"]]))\n",
    "        ])\n",
    "\n",
    "oe_gender = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\", OrdinalEncoder(categories=[[\"F\",\"M\"]]))\n",
    "        ])\n",
    "\n",
    "std_body_signals = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_std\", StandardScaler())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN TRASFORMATION\n",
    "body_signals = [\n",
    "    \"height(cm)\",\"weight(kg)\",\"waist(cm)\",\"eyesight(left)\",\"eyesight(right)\",\n",
    "    \"hearing(left)\",\"hearing(right)\",\"systolic\",\"relaxation\",\"fasting blood sugar\",\n",
    "    \"Cholesterol\",\"triglyceride\",\"HDL\",\"LDL\",\"hemoglobin\",\"Urine protein\",\n",
    "    \"serum creatinine\",\"AST\",\"ALT\",\"Gtp\"\n",
    "]\n",
    "\n",
    "smoking_tr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"id\", \"drop\", [\"ID\"]),\n",
    "        (\"gender\", oe_gender, [\"gender\"]),\n",
    "        (\"age\", minmax_age, [\"age\"]),\n",
    "        (\"body_signals\", std_body_signals, body_signals),\n",
    "        (\"oral\", \"drop\", [\"oral\"]),\n",
    "        (\"tartar\", oe_tartar, [\"tartar\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\", \n",
    "    sparse_threshold=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separiamo feature e target columns:\n",
    "X = dataset.drop(columns=[\"smoking\"])\n",
    "y = dataset[\"smoking\"]\n",
    "\n",
    "# Divisione in train e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,  stratify = y, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train==1)/len(y_train) , sum(y_test==1)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smoking_tr.fit_transform(X_train)\n",
    "X_test = smoking_tr.transform(X_test)\n",
    "\n",
    "model_pipeline = IMBPipeline([\n",
    "    ('sampler', SMOTE()),\n",
    "    ('dim_reduction', PCA(n_components=0.8)),\n",
    "    ('classifier', Perceptron())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_configs = [\n",
    "    {\n",
    "        'sampler':[None],\n",
    "    },\n",
    "    {\n",
    "        'sampler':[SMOTE()],\n",
    "        'sampler__sampling_strategy':['minority', 1.2, 0.9, 0.7]\n",
    "    },\n",
    "    {\n",
    "        'sampler':[RandomOverSampler()],\n",
    "        'sampler__sampling_strategy':['minority', 1.2, 0.9, 0.7]\n",
    "    }\n",
    "]\n",
    "\n",
    "dim_reduction_configs = [\n",
    "    {\n",
    "        'dim_reduction': [None]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [PCA()],\n",
    "        'dim_reduction__n_components': [0.5, 0.7, 0.9]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [LDA()]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [SFS(estimator=Perceptron(), cv = None, scoring = 'f1')],\n",
    "        'dim_reduction__estimator': [Perceptron(), LogisticRegression()],\n",
    "        'dim_reduction__k_features' : [5,7,10]  \n",
    "    }\n",
    "]\n",
    "\n",
    "classifier_configs = [\n",
    "    {\n",
    "        'classifier__eta' : loguniform(0.001,100),\n",
    "        'classifier': [Perceptron()],\n",
    "        'classifier__epochs': [1,5,10,15,50] ,\n",
    "    },\n",
    "    {\n",
    "        'classifier': [LogisticRegression(solver='saga')],\n",
    "        'classifier__C' : loguniform(0.001,100),\n",
    "        'classifier__penalty': ['l1','l2'],\n",
    "        'classifier__class_weight' : [None, 'balanced']\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [3,5,7,9]\n",
    "    },\n",
    "    {\n",
    "        'classifier' : [RandomForestClassifier()],\n",
    "        'classifier__n_estimators' : [10,50,100,500]\n",
    "    }\n",
    "]\n",
    "\n",
    "regression_configs = [\n",
    "    {\n",
    "        'classifier': [LinearRegression()],\n",
    "        'classifier__fit_intercept' : [True, False]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs = [dict(itertools.chain(*(e.items() \n",
    "for e in configuration))) \n",
    "for configuration in \n",
    "itertools.product(sampler_configs,dim_reduction_configs,classifier_configs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Number of all possible configurations: {len(all_configs)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(model_pipeline,\n",
    "    param_distributions=all_configs,\n",
    "    n_iter=len(all_configs) * 5,\n",
    "    n_jobs=-1,\n",
    "    cv = 2,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(rs, X_train, y_train, scoring='f1', cv = 5, return_estimator=True, verbose=3) #3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    print(estimator.best_estimator_.get_params()['sampler'])\n",
    "    print(estimator.best_estimator_.get_params()['dim_reduction'])\n",
    "    print(estimator.best_estimator_.get_params()['classifier'],estimator.best_estimator_.get_params()['classifier'].get_params())\n",
    "    print(scores['test_score'][index])\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in scores['estimator']:\n",
    "    pred_train = estimator.best_estimator_.fit(X_train, y_train)\n",
    "    pred_train = estimator.best_estimator_.predict(X_train)\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    f1_train = f1_score(y_train, pred_train)\n",
    "    f1_test = f1_score(y_test, pred_test)\n",
    "    print(f'F1 on training set:{f1_train}, F1 on test set:{f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pipeline = IMBPipeline([\n",
    "    ('trans', smoking_tr),\n",
    "    ('classifier',LogisticRegression(solver='saga', penalty = 'l1'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'classifier__C': uniform(loc = 5, scale = 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = RandomizedSearchCV(\n",
    "    estimator = best_model_pipeline,\n",
    "    param_distributions = params,\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1),\n",
    "    n_iter=20,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best.fit(X_train, y_train) #40 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, rs_best.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = rs_best.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(cls,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes= [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                                       cv = 5,\n",
    "                                                       n_jobs = -1,\n",
    "                                                       scoring = 'f1',\n",
    "                                                       shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='+',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='d', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.60, 1.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_C = [0.001,0.01,0.1,1,10,100]\n",
    "train_scores, test_scores = validation_curve(cls,\n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        param_range=\n",
    "        range_C, \n",
    "        param_name='classifier__C',\n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_C, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_C, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Parameter C')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.6, 0.8])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim([0.05,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When i want to get ad overall evaluation of my classifier model i used, i have to look at:\n",
    "ROC curve, Area under the curve and f1-score.\n",
    "\n",
    "Area under the curve: the closer it looks to a square, the better is.\n",
    "\n",
    "Attention: for ADALINE and Perceptron, we cannot evaluate ROC curve on Loss function, we can just work on f1-score (the precision-recall relation)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "e86ca8c4e97c48a29ff14cb04bf74249",
  "deepnote_persisted_session": {
   "createdAt": "2025-01-04T10:59:37.017Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
