{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MACHINE LEARING PROJECT**\n",
    "*by Brusati Lorenzo, nÂ°535355*\n",
    "\n",
    "This project aims to develop a comprehensive machine learning pipeline to predict smoking habits using various features. In the following sections, we outline the process from dataset acquisition to data preprocessing, model selection, and the final refinement of our best-performing model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "879938d90ee147fb8acd97e8c37c440a",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988329110,
    "source_hash": "7f5397b1"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Œ Standard Libraries\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "# ðŸ“Œ Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ“Œ Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "# ðŸ“Œ Scikit-Learn: Preprocessing & Transformation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures, FunctionTransformer\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# ðŸ“Œ Scikit-Learn: Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# ðŸ“Œ Scikit-Learn: Models\n",
    "from sklearn.linear_model import (\n",
    "    Perceptron, LogisticRegression, LinearRegression, SGDClassifier, \n",
    "    PassiveAggressiveClassifier, RidgeClassifier, RidgeClassifierCV\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# ðŸ“Œ Advanced Models (Boosting, Stacking, Ensemble)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlxtend.classifier import (\n",
    "    Adaline, SoftmaxRegression, MultiLayerPerceptron, StackingClassifier, \n",
    "    StackingCVClassifier, EnsembleVoteClassifier\n",
    ")\n",
    "\n",
    "# ðŸ“Œ Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "\n",
    "# ðŸ“Œ Scikit-Learn: Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, matthews_corrcoef, f1_score, classification_report, confusion_matrix, \n",
    "    accuracy_score, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# ðŸ“Œ Scikit-Learn: Model Selection & Cross Validation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, RepeatedStratifiedKFold, \n",
    "    cross_val_score, cross_validate, learning_curve, validation_curve, \n",
    "    GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "\n",
    "# ðŸ“Œ Scipy: Distributions for Hyperparameter Tuning\n",
    "from scipy.stats import loguniform, beta, uniform\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading and Analyzing the Dataset**\n",
    "\n",
    "In this section, we begin by importing the essential libraries for data handling, visualization, and machine learning. The dataset is retrieved from my personal GitHub repository using the `requests` module, which leverages a GitHub token for authentication.\n",
    "\n",
    "Key steps include:\n",
    "- **Loading Data into a DataFrame:** The CSV content is read using `pd.read_csv` after wrapping the text in a `StringIO` object, this ensures the dataset is not effectively downloaded locally but will be stored in the variable `dataset` for the whole project study.\n",
    "- **Dataset Overview:** The `dataset` call provides a visualization of the DataFrame, with the number of rows, columns, and data types, ensuring that we understand the dataset's structure before moving on to further analysis. In fact, the dataset provide column with inforations about people body features, blood testsand and a final target column that let us know if the person has smoking habits or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e981bb8fb93944619fdc3a3cd14b47b0",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 402,
    "execution_start": 1735988205697,
    "source_hash": "1f580a81"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "# file_name = \"dataset_smoking.csv\"\n",
    "file_name = \"balanced_sampled_dataset.csv\"\n",
    "file_url = f\"https://raw.githubusercontent.com/brusati04/smoking_ml_project/main/{file_name}\"\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "\n",
    "response = requests.get(file_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    from io import StringIO\n",
    "    csv_dataset = StringIO(response.text)\n",
    "    dataset = pd.read_csv(csv_dataset)\n",
    "else:\n",
    "    print(\"Download failed\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "\n",
    "Before splitting the data into training and testing sets, we perform several preprocessing steps to ensure that the dataset is clean and well-prepared for modeling. This section addresses missing values, potential imbalances, unnecessary information and data transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Target Distribution Analysis:**  \n",
    "  We compute the normalized counts of the target variable (`smoking`) using `value_counts(normalize=True)`. This analysis helps identify class imbalances which could affect model performance, fortunately the model has a perfect balance between smokers and non smokers people. This ensure the model will have enought datas to learn from, in order to correctly distinguish the binary target lablel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "55b00b90d69742ea9d744092ee323d17",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 0,
    "execution_start": 1735988206326,
    "source_hash": "bd23577c"
   },
   "outputs": [],
   "source": [
    "# Analizziamo il bilanciamento della variabile target\n",
    "distr = dataset[\"smoking\"].value_counts(normalize=True)\n",
    "print(f\"Distribuzione della variabile target: {distr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Inspection and Handling of Missing Values:**\n",
    "  Each column is examined for missing data by iterating through the columns and counting the number of `NaN` entries. Rows with more than a specified threshold of missing values (e.g., more than 2 missing values) are dropped using `dropna`. This ensures that the remaining data is robust enough for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analizziamo la distribuzione dei Nan values:\n",
    "print(\"Missing values in the dataset:\")\n",
    "for col in dataset.columns:\n",
    "    nan_count = dataset[col].isnull().sum()\n",
    "    print(f\"{col}: {nan_count}\")\n",
    "\n",
    "# Set threshold\n",
    "n = 2\n",
    "num_rows = (dataset.isna().sum(axis=1) > n).sum()\n",
    "\n",
    "dataset = dataset.dropna(axis=0, thresh=len(dataset.columns)-2)\n",
    "\n",
    "print(f\"Number of rows with more than {n} NaN values: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Dominant Category Check:** \n",
    "  For each column, the ratio of the most frequent category is computed. Columns where a single category dominates (ratio greater than 0.9) are dropped, reducing bias and redundancy in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of the most common category to all columns\n",
    "dominant_ratios = dataset.apply(lambda series: series.value_counts(normalize=True).iloc[0])\n",
    "print(dominant_ratios)\n",
    "\n",
    "# Let's drop the columns with a dominant category ratio greater than 0.9:\n",
    "dataset = dataset.drop(columns=dominant_ratios[dominant_ratios > 0.9].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Visualization of Missing Data:**  \n",
    "  The `missingno` library is used to generate a matrix plot that visually represents the presence of missing values. This visualization assists in understanding the overall data quality that we reached until now and pointing areas that require further attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Data Transformation Pipeline Setup:**  \n",
    "  A series of transformations is prepared using scikit-learn `Pipeline`:\n",
    "\n",
    "  - **Scaling Numerical Features:** The `age` column is scaled using `MinMaxScaler` to normalize its range.\n",
    "  \n",
    "  - **Encoding Categorical Variables:**  For categorical features such as `gender` and `tartar`, a two-step process is implemented:\n",
    "    1. **Imputation:** Missing values are first handled using `SimpleImputer` with the strategy set to \"most_frequent\".\n",
    "    \n",
    "    2. **Ordinal Encoding:** The `OrdinalEncoder` is then applied, converting the categorical values into numerical form. The encoding is explicitly defined (e.g., `[\"F\", \"M\"]` for gender and `[\"N\", \"Y\"]` for tartar) to maintain consistency.\n",
    "  \n",
    "  - **Standardizing Body Signals:** A similar pipeline is constructed for all the blood tests columns features, ensuring they are standardized for better model performance with `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8b656d0c6e9d4801b9736dd28ceee4b1",
    "deepnote_cell_type": "code",
    "execution_context_id": "986dc820-10f0-4ed9-b446-6159801ba3a7",
    "execution_millis": 875,
    "execution_start": 1735988206406,
    "source_hash": "bf555741"
   },
   "outputs": [],
   "source": [
    "minmax_age = MinMaxScaler()\n",
    "\n",
    "oe_tartar = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\",  OrdinalEncoder(categories=[[\"N\",\"Y\"]]))\n",
    "        ])\n",
    "\n",
    "oe_gender = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_ord\", OrdinalEncoder(categories=[[\"F\",\"M\"]]))\n",
    "        ])\n",
    "\n",
    "body_signals = dataset.columns.drop([\"ID\", \"gender\", \"age\" ,\"dental caries\", \"tartar\", \"smoking\"]).tolist()\n",
    "\n",
    "std_body_signals = Pipeline([\n",
    "        (\"pipe_sim\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"pipe_std\", StandardScaler())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Combining Transformations:**  \n",
    "  All individual pipelines are integrated into a single `ColumnTransformer` that applies the appropriate transformation to each set of columns. This unified approach ensures consistency and simplifies the application of preprocessing steps to both the training and testing datasets.\n",
    "  remaining features will be passed thanks to `remainder=\"passthrough\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN TRASFORMATION\n",
    "\n",
    "smoking_tr = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"id\", \"drop\", [\"ID\"]),\n",
    "        (\"gender\", oe_gender, [\"gender\"]),\n",
    "        (\"age\", minmax_age, [\"age\"]),\n",
    "        (\"body_signals\", std_body_signals, body_signals),\n",
    "        (\"tartar\", oe_tartar, [\"tartar\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\", # not applying anythig to dental caries \n",
    "    sparse_threshold=1,\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - **Data Transformation Pipeline Setup:**  \n",
    "Here we provide a final visualization of the dataset, just to check that all the transformation worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_datatset = pd.DataFrame(smoking_tr.fit_transform(dataset), columns=smoking_tr.get_feature_names_out())\n",
    "final_datatset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train and Test Split**\n",
    "\n",
    "Now let's finally jump right into where the real work begins. After preprocessing, the next critical step is to split the dataset into training and testing sets. This section outlines how we separate the data and prepare it for model training and evaluation:\n",
    "\n",
    "- **Feature and Target column Separation:**  \n",
    "  The dataset is divided into features (`X`) and the target variable (`y`). The target is the `smoking` column, that is a binary 0-1 column. so we are dealing with \n",
    "\n",
    "- **Splitting the Data:**  \n",
    "  The `train_test_split` function from scikit-learn is used to divide the data. The key parameters include:\n",
    "  - `test_size`: percentage of the data reserved for testing.\n",
    "  - `stratify=y`: Ensures that the target variableâ€™s distribution is maintained in both the training and testing sets, which is especially important when dealing with imbalanced classes.\n",
    "  - `random_state=42` and `shuffle=True`: These parameters guarantee reproducibility and ensure iid: indipendent and identically distributed (shuffling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separiamo feature e target columns:\n",
    "X = dataset.drop(columns=[\"smoking\"])\n",
    "y = dataset[\"smoking\"]\n",
    "\n",
    "# Divisione in train e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train==1)/len(y_train) , sum(y_test==1)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Applying the Transformation Pipeline:**  \n",
    "  After splitting, the transformation pipeline `smoking_tr` is applied to both the training and test sets. From now on, we assume that our data have been correctly preprocessed and transformed.\n",
    "\n",
    "- **Initial Pipeline Setup:**  \n",
    "  We have to define the candidate models and select the best candidates through the nested-cross validation process which combine hyperparameter optimization and model selection into a single block of code. In order to do that, we configured a model pipeline using an imbalanced data handling pipeline (`IMBPipeline`), which incorporates:\n",
    "  - **Sampler:** for handling class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "  - **Dimensionality Reduction:** for reducing feature dimensionality and improving computational efficiency.\n",
    "  - **Classifier:** for making predictions based on the transformed dataset.\n",
    "\n",
    "Let's proceed by defining a starting model or pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smoking_tr.fit_transform(X_train)\n",
    "X_test = smoking_tr.transform(X_test)\n",
    "\n",
    "model_pipeline = IMBPipeline([\n",
    "    ('sampler', None),\n",
    "    ('dim_reduction', None),\n",
    "    ('classifier', None)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**\n",
    "\n",
    "This section is dedicated to exploring various machine learning models and their configurations. Given the complexity of the task, multiple aspects of the model pipeline are tuned:\n",
    "\n",
    "- **Pipeline Components for Model Tuning:**\n",
    "  - **Sampler Configurations:**  \n",
    "    Different strategies are tested, including:\n",
    "    - No sampling.\n",
    "    - SMOTE with various sampling strategies (e.g., focusing on the minority class or specific ratios).\n",
    "    - RandomOverSampler with similar strategies.\n",
    "  \n",
    "  - **Dimensionality Reduction Techniques:**  \n",
    "    Multiple methods are evaluated:\n",
    "    - No dimensionality reduction.\n",
    "    - PCA with different levels of variance retention.\n",
    "    - Linear Discriminant Analysis (LDA).\n",
    "    - Sequential Feature Selector (SFS) using Logistic Regression with a cross-validation approach for feature selection.\n",
    "  \n",
    "  - **Classifier Options and Their Hyperparameters:**  \n",
    "    A wide range of classifiers is considered, such as:\n",
    "    - **SGDClassifier:** With hyperparameters like `alpha` (controlled via a log-uniform distribution) and varying numbers of iterations.\n",
    "    - **Logistic Regression:** Tuned with different regularization strengths (`C`), penalties (`l1`, `l2`), and class weight options.\n",
    "    - **K-Nearest Neighbors:** Varying the number of neighbors.\n",
    "    - **Random Forest:** Testing different numbers of estimators and maximum tree depths.\n",
    "    - **Support Vector Classifier (SVC):** With various `C` values and kernel choices.\n",
    "    - **XGBoost and LightGBM:** Advanced boosting methods with tuning for the number of estimators, learning rate, and maximum depth.\n",
    "\n",
    "- **Comprehensive Model Evaluation:**  \n",
    "  The different configurations are combined using `itertools.product` to generate a complete grid of possible settings.  \n",
    "  - **RandomizedSearchCV:**  \n",
    "    This search strategy is employed to efficiently explore the hyperparameter space by sampling a fixed number of candidates from the entire grid. The search is optimized using cross-validation (with the F1 score as the performance metric) to ensure that the model generalizes well.\n",
    "  - **Cross-Validation:**  \n",
    "    The cross-validation results, including the best configurations for each component (sampler, dimensionality reduction, and classifier), are analyzed in detail. The F1 scores for both training and test sets are computed to assess model performance and stability.\n",
    "\n",
    "This multi-faceted approach in model selection allows us to systematically evaluate and compare various pipelines and choose the best configuration for predicting the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_configs = [\n",
    "    {'sampler': [None]},\n",
    "    {'sampler': [SMOTE()], 'sampler__sampling_strategy': ['minority', 1.2, 0.9, 0.7]},\n",
    "    {'sampler': [RandomOverSampler()], 'sampler__sampling_strategy': ['minority', 1.2, 0.9, 0.7]},\n",
    "]\n",
    "\n",
    "dim_reduction_configs = [\n",
    "    {'dim_reduction': [None]},\n",
    "    {'dim_reduction': [LDA()]},\n",
    "    {'dim_reduction': [PCA()], 'dim_reduction__n_components': [None, 0.95, 0.99]}\n",
    "]\n",
    "\n",
    "classifier_configs = [\n",
    "    {\n",
    "        'classifier': [Perceptron()],\n",
    "        'classifier__max_iter': [100, 500, 1000],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__eta0': [1, 0.1, 0.01, 0.001],\n",
    "        'classifier__early_stopping': [True],\n",
    "    },\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__n_estimators': [100, 200, 500, 1000, None],\n",
    "        'classifier__max_depth': [10, 20, 30, None]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [AdaBoostClassifier()],\n",
    "        'classifier__estimator' : [DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=42)], \n",
    "        'classifier__n_estimators': [10, 100, 1000],\n",
    "        'classifier__learning_rate':[0.01,0.1,1]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [LogisticRegression()],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'classifier__solver': ['saga', 'lbfgs'],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__max_iter': [None, 100, 500, 1000]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear'],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'classifier__tol': [1e-4, 1e-3, 1e-2],\n",
    "        'classifier__max_iter': [100, 1000, 10000, 10000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs = [dict(itertools.chain(*(e.items() \n",
    "for e in configuration)))\n",
    "for configuration in itertools.product(sampler_configs,dim_reduction_configs,classifier_configs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Number of all possible configurations: {len(all_configs)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(model_pipeline,\n",
    "    param_distributions=all_configs,\n",
    "    n_iter=len(all_configs) * 10,\n",
    "    n_jobs=-1,\n",
    "    cv = 2,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=rs, X=X_train, y=y_train, scoring='f1', cv = 5, return_estimator=True, verbose=3) # 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    print(estimator.best_estimator_.get_params()['sampler'])\n",
    "    print(estimator.best_estimator_.get_params()['dim_reduction'])\n",
    "    print(estimator.best_estimator_.get_params()['classifier'])\n",
    "    print(scores['test_score'][index])\n",
    "    print(f\"Training time: {scores['fit_time'][index]:.2f} sec\")\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in scores['estimator']:\n",
    "    # Extract the model name\n",
    "    model_name = estimator.best_estimator_.get_params()['classifier'].__class__.__name__\n",
    "\n",
    "    # Train the model and predict on training and test sets\n",
    "    pred_train = estimator.best_estimator_.fit(X_train, y_train).predict(X_train)\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Calculate F1-scores\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')  # Use 'weighted' for multiclass\n",
    "    f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    cm_test = confusion_matrix(y_test, pred_test)\n",
    "    accuracy_test = accuracy_score(y_test, pred_test)\n",
    "    precision_test = precision_score(y_test, pred_test, average='weighted')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'F1 on Training Set: {f1_train:.2f}, F1 on Test Set: {f1_test:.2f}')\n",
    "    print(f'Accuracy on Test Set: {accuracy_test:.2f}')\n",
    "    print(f'Precision on Test Set: {precision_test:.2f}')\n",
    "    print('Confusion Matrix (Test Set):')\n",
    "    print(cm_test)\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Refinement of the Selected Model**\n",
    "\n",
    "After selecting the best performing model through an extensive hyperparameter tuning process, we further refine it to gain deeper insights into its behavior and robustness. In this section, we:\n",
    "\n",
    "- **Generate Learning Curves:**  \n",
    "  By computing the learning curve, we evaluate how the model's F1-score changes as the size of the training set increases. This visualization helps us determine whether the model is underfitting or overfitting, and it provides guidance on whether more training data might improve performance.\n",
    "\n",
    "- **Plot Validation Curves:**  \n",
    "  A validation curve is produced for the hyperparameter `C` (used in Logistic Regression) to assess the sensitivity of the model's performance to changes in this parameter. The curve, accompanied by confidence intervals (using standard deviation), illustrates the impact of regularization strength on both training and validation scores.  \n",
    "  This detailed analysis ensures that the selected model not only performs well on the training data but also generalizes effectively to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pipeline = IMBPipeline([\n",
    "    ('sampler', None),\n",
    "    ('dim_reduction', None),\n",
    "    ('classifier', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__kernel': ['linear'],\n",
    "    'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'classifier__tol': [1e-4, 1e-3, 1e-2],\n",
    "    'classifier__max_iter': [100, 1000, 10000, 10000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = RandomizedSearchCV(\n",
    "    estimator= best_model_pipeline,\n",
    "    param_distributions = params,\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1),\n",
    "    n_iter=20,\n",
    "    scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_best.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, rs_best.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = rs_best.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Analysis of the Graphs**\n",
    "\n",
    "In this section, we take a closer look at the visualizations generated during model refinement and performance evaluation:\n",
    "\n",
    "- **Learning Curves:**  \n",
    "  The learning curves illustrate how the F1-score evolves as the training set size increases. The training curve's steady convergence indicates that the model is capturing the underlying patterns well. However, the gap between the training and validation curves suggests a minor degree of overfitting. This gap could potentially be reduced by incorporating additional data or applying stronger regularization techniques.\n",
    "\n",
    "- **Validation Curves:**  \n",
    "  By plotting the validation curve for the hyperparameter (e.g., parameter `C` in Logistic Regression), we observe the model's sensitivity to changes in regularization strength. The curve highlights an optimal range where the performance peaks, providing clear guidance on tuning this parameter. The confidence intervals around these curves also reflect the model's stability across different folds.\n",
    "\n",
    "- **Other Visual Metrics:**  \n",
    "  Additional plots, such as confusion matrices and ROC curves, offer insights into the modelâ€™s classification performance. The confusion matrix confirms that the model is accurately distinguishing between the classes, while the ROC curve and precision-recall plots demonstrate strong performance even in the presence of class imbalance.\n",
    "\n",
    "Overall, the visual analysis confirms that while the model performs robustly, fine-tuning and further data augmentation could enhance its generalization capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(cls,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes= [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                                       cv = 5,\n",
    "                                                       n_jobs = -1,\n",
    "                                                       scoring = 'f1',\n",
    "                                                       shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='+',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='d', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.7, 0.9])\n",
    "ax.set_xlim([train_sizes[1], train_sizes[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, cls.predict(X_test)))\n",
    "matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_C = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "train_scores, test_scores = validation_curve(cls,\n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        param_range= range_C, \n",
    "        param_name='classifier__C',\n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        scoring='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_C, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_C, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation F1')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Parameter C')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.4, 0.85])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim([min(range_C), max(range_C)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When i want to get ad overall evaluation of my classifier model i used, i have to look at:\n",
    "ROC curve, Area under the curve and f1-score.\n",
    "\n",
    "Area under the curve: the closer it looks to a square, the better is.\n",
    "\n",
    "Attention: for ADALINE and Perceptron, we cannot evaluate ROC curve on Loss function, we can just work on f1-score (the precision-recall relation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusions**\n",
    "\n",
    "This project has successfully built a comprehensive machine learning pipeline to predict smoking habits using a medium-size dataset. The end-to-end workflowâ€”from data loading and preprocessing to model selection, tuning, and final evaluationâ€”has demonstrated several key insights:\n",
    "\n",
    "- **Robust Data Handling:**  \n",
    "  The initial exploratory analysis and rigorous preprocessing steps, including missing value imputation and the thoughtful design of a data transformation pipeline, ensured that the input data was of high quality. This foundational work is crucial for the success of any predictive model.\n",
    "\n",
    "- **Diverse Model Exploration:**  \n",
    "  By testing multiple classifiers and hyperparameter configurations (from SGD and Logistic Regression to ensemble methods like Random Forest and advanced boosting algorithms), the project provided a rich comparative analysis. This not only highlighted the strengths and weaknesses of different approaches but also helped identify the optimal model configuration.\n",
    "\n",
    "- **Visual Insights for Model Refinement:**  \n",
    "  The learning and validation curves, along with other graphical analyses, provided actionable insights into the model's performance. For instance, the slight divergence between training and validation performance pointed to potential areas for improvement, such as reducing overfitting through enhanced regularization or expanding the dataset.\n",
    "\n",
    "- **Practical Applications and Future Enhancements:**  \n",
    "  Beyond the technical achievements, this project has significant real-world implications. The predictive model could be used in healthcare settings for early identification of individuals at risk, enabling targeted interventions and personalized health recommendations. Additionally, the methodology could be adapted for related fields such as lifestyle analytics, public health monitoring, and even insurance risk assessment.\n",
    "\n",
    "Looking ahead, future work could focus on:\n",
    "- **Enhanced Feature Engineering:** Incorporating additional data sources or domain-specific features to further improve model accuracy.\n",
    "- **Advanced Modeling Techniques:** Exploring deep learning architectures or more sophisticated ensemble methods to capture non-linear relationships.\n",
    "\n",
    "In summary, while the current results are promising, there remains ample opportunity for refinement and extension, ensuring that the model not only performs well in a controlled setting but also delivers practical value in real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "e86ca8c4e97c48a29ff14cb04bf74249",
  "deepnote_persisted_session": {
   "createdAt": "2025-01-04T10:59:37.017Z"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
